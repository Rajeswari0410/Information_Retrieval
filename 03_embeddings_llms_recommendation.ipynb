{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAxczUGVN-Jc"
      },
      "source": [
        "# Advanced Recommendation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywt9ohHtL77T"
      },
      "source": [
        "# Part 1. Non-personalized Recommendations with User Ratings\n",
        "\n",
        "In this first part, we're going to build a non-personalized recommender based on user ratings.  In many online platforms, such as Amazon, IMDb, and MovieLens, users are able to express their preference to items by explicit ratings (like by assigning a 1-5 star rating to a movie). We're going to use those ratings to generate a recommendation. For this part, we're focusing on **non-personalized** recommendations (that is, everyone gets the same recommendation).\n",
        "\n",
        "For this part, we will:\n",
        "\n",
        "* load and process the MovieLens 1M dataset,\n",
        "* build the non-personalized recommender, and\n",
        "* evaluate the recommender."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k012MhyfXysi",
        "outputId": "e965f750-fbd4-4208-9f79-19b9afe71030"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZaocWZ94MD-e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "path = \"/content/drive/MyDrive/ML_633/ratings-1.dat\"\n",
        "data_df = pd.read_csv(path, sep='::', names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], engine='python')\n",
        "\n",
        "# First, generate dictionaries for mapping old id to new id for users and movies\n",
        "unique_MovieID = data_df['MovieID'].unique()\n",
        "unique_UserID = data_df['UserID'].unique()\n",
        "j = 0\n",
        "user_old2new_id_dict = dict()\n",
        "for u in unique_UserID:\n",
        "    user_old2new_id_dict[u] = j\n",
        "    j += 1\n",
        "j = 0\n",
        "movie_old2new_id_dict = dict()\n",
        "for i in unique_MovieID:\n",
        "    movie_old2new_id_dict[i] = j\n",
        "    j += 1\n",
        "\n",
        "# Then, use the generated dictionaries to reindex UserID and MovieID in the data_df\n",
        "user_list = data_df['UserID'].values\n",
        "movie_list = data_df['MovieID'].values\n",
        "for j in range(len(data_df)):\n",
        "    user_list[j] = user_old2new_id_dict[user_list[j]]\n",
        "    movie_list[j] = movie_old2new_id_dict[movie_list[j]]\n",
        "data_df['UserID'] = user_list\n",
        "data_df['movieID'] = movie_list\n",
        "\n",
        "# generate train_df with 70% samples and test_df with 30% samples, and there should have no overlap between them.\n",
        "train_index = np.random.random(len(data_df)) <= 0.7\n",
        "train_df = data_df[train_index]\n",
        "test_df = data_df[~train_index]\n",
        "\n",
        "# generate train_mat and test_mat\n",
        "num_user = len(data_df['UserID'].unique())\n",
        "num_movie = len(data_df['MovieID'].unique())\n",
        "\n",
        "train_mat = coo_matrix((train_df['Rating'].values, (train_df['UserID'].values, train_df['MovieID'].values)), shape=(num_user, num_movie)).astype(float).toarray()\n",
        "test_mat = coo_matrix((test_df['Rating'].values, (test_df['UserID'].values, test_df['MovieID'].values)), shape=(num_user, num_movie)).astype(float).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSl0jPBiL77U"
      },
      "source": [
        "## Part 1a: Build the non-personalized recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWS-5RPyL77U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "movie_avgs = np.divide(np.sum(train_mat, axis=0), np.count_nonzero(train_mat, axis=0), where=np.count_nonzero(train_mat, axis=0) != 0)\n",
        "total_avgs = np.sum(np.sum(train_mat, axis=0)) / np.sum(np.count_nonzero(train_mat, axis=0))\n",
        "prediction_mat = np.full((num_user, num_movie), total_avgs)\n",
        "\n",
        "for i in range(num_movie):\n",
        "    if movie_avgs[i] > 0:\n",
        "        prediction_mat[:, i] = movie_avgs[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKCUS66AL77U"
      },
      "source": [
        "Please print out the id of the top-5 movies with largest predicted ratings and their predicted ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3au2ezoL77U",
        "outputId": "cd575aee-0a21-4198-9689-8f4556b1836a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-5 Movies with Largest Predicted Ratings and their predicted ratings:\n",
            "Movie ID: 3382, Predicted Rating: 5.00\n",
            "Movie ID: 439, Predicted Rating: 5.00\n",
            "Movie ID: 3607, Predicted Rating: 5.00\n",
            "Movie ID: 3881, Predicted Rating: 5.00\n",
            "Movie ID: 2930, Predicted Rating: 5.00\n"
          ]
        }
      ],
      "source": [
        "user1 = prediction_mat[0, :]\n",
        "top_5_idx = np.argsort(user1)[-5:][::-1]\n",
        "top_5_ratings = user1[top_5_idx]\n",
        "print(\"Top-5 Movies with Largest Predicted Ratings and their predicted ratings:\")\n",
        "for idx, movie_idx in enumerate(top_5_idx):\n",
        "    # Convert new movie index back to the original movie ID\n",
        "    original_movie_id = list(movie_old2new_id_dict.keys())[list(movie_old2new_id_dict.values()).index(movie_idx)]\n",
        "    print(f\"Movie ID: {original_movie_id}, Predicted Rating: {top_5_ratings[idx]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqtYuyaxL77U"
      },
      "source": [
        "## Part 1b: Evaluate the non-personalized recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_apvbmNL77U",
        "outputId": "9434fe42-93b0-4a11-f60d-4e658953ff08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE: 0.9793182255274792\n"
          ]
        }
      ],
      "source": [
        "# calculate and print out the RMSE for your prediction_df and the test_df\n",
        "sq_error = (prediction_mat[test_mat.nonzero()] - test_mat[test_mat.nonzero()]) ** 2\n",
        "rmse = np.sqrt(np.mean(sq_error))\n",
        "print(\"RMSE:\", rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKZ8ijNPTXMN"
      },
      "source": [
        "# Part 2. Our own Netflix Prize (sort of)\n",
        "\n",
        "* Try item-item collaborative filtering instead of user-user CF\n",
        "* Try to include the baseline estimation model in your collaborative filtering model\n",
        "* Build an MF model\n",
        "* Add bias factors to your MF model and learn them\n",
        "* Add CF to your MF model and learn the CF weights\n",
        "* Incorporate an LLM (?) into your model\n",
        "* ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VFzzcbv4KRx"
      },
      "outputs": [],
      "source": [
        "#item-item CF with KNN\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# cosine similarity between items\n",
        "item_sim = cosine_similarity(train_mat.T)\n",
        "k = 7\n",
        "knn_ii_pred = np.zeros((num_user, num_movie))\n",
        "\n",
        "for user in range(num_user):\n",
        "    for movie in range(num_movie):\n",
        "        sim_scores = item_sim[movie]\n",
        "        rated_movies = np.where(train_mat[user] > 0)[0]\n",
        "        if len(rated_movies) == 0:\n",
        "            continue\n",
        "        rated_sim = sim_scores[rated_movies]\n",
        "        if len(rated_sim) > k:\n",
        "            top_k_idx = np.argsort(rated_sim)[-k:]\n",
        "        else:\n",
        "            top_k_idx = np.argsort(rated_sim)\n",
        "\n",
        "        top_k_movies = rated_movies[top_k_idx]\n",
        "        top_k_sims = rated_sim[top_k_idx]\n",
        "        top_k_ratings = train_mat[user, top_k_movies]\n",
        "\n",
        "        if np.sum(np.abs(top_k_sims)) > 0:\n",
        "            knn_ii_pred[user, movie] = np.dot(top_k_sims, top_k_ratings) / np.sum(np.abs(top_k_sims))\n",
        "        else:\n",
        "            knn_ii_pred[user, movie] = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCP_RxkEnVwf",
        "outputId": "4c06ce4e-c4e3-46df-e3fc-4c9899aa8953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE of item-item CF with KNN: 0.9441\n"
          ]
        }
      ],
      "source": [
        "# print your best RMSE for the test set\n",
        "sq_error = (knn_ii_pred[test_mat.nonzero()] - test_mat[test_mat.nonzero()]) ** 2\n",
        "rmse = np.sqrt(np.mean(sq_error))\n",
        "print(f\"RMSE of item-item CF with KNN: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFsG-Q26hyPF",
        "outputId": "ecd3d5bb-4776-4484-f5e9-0f7ac9e7e36a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[4.92649175 3.99390152 4.71326001 ... 1.54431024 5.54431024 4.54431024]\n",
            " [4.46464965 3.53205941 4.2514179  ... 1.08246813 5.08246813 4.08246813]\n",
            " [4.71815842 3.78556818 4.50492667 ... 1.3359769  5.3359769  4.3359769 ]\n",
            " ...\n",
            " [4.61399175 3.68140152 4.40076001 ... 1.23181024 5.23181024 4.23181024]\n",
            " [4.57072252 3.63813229 4.35749078 ... 1.18854101 5.18854101 4.18854101]\n",
            " [4.35468324 3.42209301 4.1414515  ... 0.97250173 4.97250173 3.97250173]]\n"
          ]
        }
      ],
      "source": [
        "# Baseline estimation in CF (bias)\n",
        "mu = np.mean(train_mat[train_mat > 0])\n",
        "bu = np.zeros(train_mat.shape[0])\n",
        "for user in range(train_mat.shape[0]):\n",
        "    user_ratings = train_mat[user, :]\n",
        "    rated_indices = user_ratings > 0\n",
        "    if rated_indices.sum() > 0:\n",
        "        bu[user] = np.mean(user_ratings[rated_indices]) - mu\n",
        "\n",
        "bi = np.zeros(train_mat.shape[1])\n",
        "for movie in range(train_mat.shape[1]):\n",
        "    movie_ratings = train_mat[:, movie]\n",
        "    rated_indices = movie_ratings > 0\n",
        "    if rated_indices.sum() > 0:\n",
        "        bi[movie] = np.mean(movie_ratings[rated_indices]) - mu\n",
        "\n",
        "# Predict the rating: b_ui = mu + bi + bu\n",
        "baseline_pred = mu + bu[:, np.newaxis] + bi\n",
        "print(baseline_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_ZRoWV8Lbg2",
        "outputId": "3f91ad0e-42d6-426a-b35a-8bb449e23b02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE of the Baseline: 0.9340\n"
          ]
        }
      ],
      "source": [
        "sq_error = (baseline_pred[test_mat.nonzero()] - test_mat[test_mat.nonzero()]) ** 2\n",
        "rmse = np.sqrt(np.mean(sq_error))\n",
        "print(f\"RMSE of the Baseline: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dxfpwu8Eh4yG",
        "outputId": "29f660eb-a7b6-41cc-eec4-e1a48c468812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, RMSE: 2.6768\n",
            "Epoch 2/20, RMSE: 1.1050\n",
            "Epoch 3/20, RMSE: 0.9691\n",
            "Epoch 4/20, RMSE: 0.9477\n",
            "Epoch 5/20, RMSE: 0.9399\n",
            "Epoch 6/20, RMSE: 0.9346\n",
            "Epoch 7/20, RMSE: 0.9294\n",
            "Epoch 8/20, RMSE: 0.9238\n",
            "Epoch 9/20, RMSE: 0.9183\n",
            "Epoch 10/20, RMSE: 0.9132\n",
            "Epoch 11/20, RMSE: 0.9087\n",
            "Epoch 12/20, RMSE: 0.9047\n",
            "Epoch 13/20, RMSE: 0.9012\n",
            "Epoch 14/20, RMSE: 0.8979\n",
            "Epoch 15/20, RMSE: 0.8948\n",
            "Epoch 16/20, RMSE: 0.8919\n",
            "Epoch 17/20, RMSE: 0.8890\n",
            "Epoch 18/20, RMSE: 0.8863\n",
            "Epoch 19/20, RMSE: 0.8837\n",
            "Epoch 20/20, RMSE: 0.8812\n"
          ]
        }
      ],
      "source": [
        "# MF\n",
        "def MF(train_mat, num_factors=9, lr=0.01, reg=0.001, epochs=20):\n",
        "    num_users, num_items = train_mat.shape\n",
        "    # Initialize latent factors\n",
        "    User = np.random.normal(scale=0.1, size=(num_users, num_factors))\n",
        "    Item = np.random.normal(scale=0.1, size=(num_items, num_factors))\n",
        "    rows, cols = train_mat.nonzero()\n",
        "    ratings = train_mat[rows, cols]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(len(ratings)):\n",
        "            u = rows[i]\n",
        "            m = cols[i]\n",
        "            rating = ratings[i]\n",
        "            pred = np.dot(User[u], Item[m])\n",
        "            error = rating - pred\n",
        "            # Update latent vectors using SGD\n",
        "            User[u] += lr * (error * Item[m] - reg * User[u])\n",
        "            Item[m] += lr * (error * User[u] - reg * Item[m])\n",
        "\n",
        "        MF_pred = User @ Item.T\n",
        "        train_loss = np.sqrt(np.mean((train_mat[rows, cols] - MF_pred[rows, cols])**2))\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, RMSE: {train_loss:.4f}\")\n",
        "\n",
        "    return User @ Item.T\n",
        "\n",
        "mf_pred = MF(train_mat, num_factors=9, lr=0.01, reg=0.1, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcBY7xr2zWPV",
        "outputId": "a85ec192-c108-4af1-9c1f-4f59aae0b894"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE of the MF: 0.9103\n"
          ]
        }
      ],
      "source": [
        "sq_error = (mf_pred[test_mat.nonzero()] - test_mat[test_mat.nonzero()]) ** 2\n",
        "rmse = np.sqrt(np.mean(sq_error))\n",
        "print(f\"RMSE of the MF: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WbSRjsth7TW",
        "outputId": "d40f730d-c1a9-42bb-8238-e870fb4feff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, RMSE: 0.9362\n",
            "Epoch 2/50, RMSE: 0.9141\n",
            "Epoch 3/50, RMSE: 0.9079\n",
            "Epoch 4/50, RMSE: 0.9050\n",
            "Epoch 5/50, RMSE: 0.9031\n",
            "Epoch 6/50, RMSE: 0.9015\n",
            "Epoch 7/50, RMSE: 0.8998\n",
            "Epoch 8/50, RMSE: 0.8975\n",
            "Epoch 9/50, RMSE: 0.8945\n",
            "Epoch 10/50, RMSE: 0.8908\n",
            "Epoch 11/50, RMSE: 0.8869\n",
            "Epoch 12/50, RMSE: 0.8833\n",
            "Epoch 13/50, RMSE: 0.8800\n",
            "Epoch 14/50, RMSE: 0.8771\n",
            "Epoch 15/50, RMSE: 0.8745\n",
            "Epoch 16/50, RMSE: 0.8719\n",
            "Epoch 17/50, RMSE: 0.8694\n",
            "Epoch 18/50, RMSE: 0.8669\n",
            "Epoch 19/50, RMSE: 0.8644\n",
            "Epoch 20/50, RMSE: 0.8619\n",
            "Epoch 21/50, RMSE: 0.8595\n",
            "Epoch 22/50, RMSE: 0.8571\n",
            "Epoch 23/50, RMSE: 0.8548\n",
            "Epoch 24/50, RMSE: 0.8526\n",
            "Epoch 25/50, RMSE: 0.8505\n",
            "Epoch 26/50, RMSE: 0.8485\n",
            "Epoch 27/50, RMSE: 0.8466\n",
            "Epoch 28/50, RMSE: 0.8447\n",
            "Epoch 29/50, RMSE: 0.8429\n",
            "Epoch 30/50, RMSE: 0.8411\n",
            "Epoch 31/50, RMSE: 0.8394\n",
            "Epoch 32/50, RMSE: 0.8378\n",
            "Epoch 33/50, RMSE: 0.8362\n",
            "Epoch 34/50, RMSE: 0.8347\n",
            "Epoch 35/50, RMSE: 0.8332\n",
            "Epoch 36/50, RMSE: 0.8317\n",
            "Epoch 37/50, RMSE: 0.8303\n",
            "Epoch 38/50, RMSE: 0.8289\n",
            "Epoch 39/50, RMSE: 0.8276\n",
            "Epoch 40/50, RMSE: 0.8263\n",
            "Epoch 41/50, RMSE: 0.8250\n",
            "Epoch 42/50, RMSE: 0.8238\n",
            "Epoch 43/50, RMSE: 0.8226\n",
            "Epoch 44/50, RMSE: 0.8214\n",
            "Epoch 45/50, RMSE: 0.8203\n",
            "Epoch 46/50, RMSE: 0.8191\n",
            "Epoch 47/50, RMSE: 0.8181\n",
            "Epoch 48/50, RMSE: 0.8170\n",
            "Epoch 49/50, RMSE: 0.8160\n",
            "Epoch 50/50, RMSE: 0.8150\n"
          ]
        }
      ],
      "source": [
        "# MF with Bias\n",
        "def MF_bias(train_mat, num_factors=9, lr=0.01, reg=0.01, epochs=20):\n",
        "    num_users, num_items = train_mat.shape\n",
        "    # Initialize latent factors\n",
        "    User = np.random.normal(scale=0.1, size=(num_users, num_factors))\n",
        "    Item = np.random.normal(scale=0.1, size=(num_items, num_factors))\n",
        "    #Initialize bias\n",
        "    ub = np.zeros(num_users)\n",
        "    ib = np.zeros(num_items)\n",
        "    mu = np.mean(train_mat[train_mat != 0])\n",
        "    rows, cols = train_mat.nonzero()\n",
        "    ratings = train_mat[rows, cols]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(len(ratings)):\n",
        "            u = rows[i]\n",
        "            m = cols[i]\n",
        "            rating = ratings[i]\n",
        "            pred = mu + ub[u] + ib[m] + np.dot(User[u], Item[m])\n",
        "            error = rating - pred\n",
        "            # Update latent vectors using SGD\n",
        "            ub[u] += lr * (error - reg * ub[u])\n",
        "            ib[m] += lr * (error - reg * ib[m])\n",
        "            User[u] += lr * (error * Item[m] - reg * User[u])\n",
        "            Item[m] += lr * (error * User[u] - reg * Item[m])\n",
        "\n",
        "        MF_bias_pred = mu + ub[:, np.newaxis] + ib[np.newaxis,:] + User @ Item.T\n",
        "        train_loss = np.sqrt(np.mean((train_mat[rows, cols] - MF_bias_pred[rows, cols])**2))\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, RMSE: {train_loss:.4f}\")\n",
        "\n",
        "    return mu + ub[:, np.newaxis] + ib[np.newaxis,:] + User @ Item.T\n",
        "\n",
        "mf_bias_pred = MF_bias(train_mat, num_factors=25, lr=0.01, reg=0.1, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MljlR-Kx3nf",
        "outputId": "5e65b76e-c25d-4793-c990-b1d6696ea615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE of the MF: 0.8714\n"
          ]
        }
      ],
      "source": [
        "sq_error = (mf_bias_pred[test_mat.nonzero()] - test_mat[test_mat.nonzero()]) ** 2\n",
        "rmse = np.sqrt(np.mean(sq_error))\n",
        "print(f\"RMSE of the MF: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwGIXNfFh9zR",
        "outputId": "857727a8-c7d2-48a0-a52d-5fb3eb5b4da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25, RMSE: 0.9428\n",
            "Epoch 2/25, RMSE: 0.9176\n",
            "Epoch 3/25, RMSE: 0.9069\n",
            "Epoch 4/25, RMSE: 0.9008\n",
            "Epoch 5/25, RMSE: 0.8966\n",
            "Epoch 6/25, RMSE: 0.8933\n",
            "Epoch 7/25, RMSE: 0.8903\n",
            "Epoch 8/25, RMSE: 0.8874\n",
            "Epoch 9/25, RMSE: 0.8842\n",
            "Epoch 10/25, RMSE: 0.8805\n",
            "Epoch 11/25, RMSE: 0.8761\n",
            "Epoch 12/25, RMSE: 0.8712\n",
            "Epoch 13/25, RMSE: 0.8658\n",
            "Epoch 14/25, RMSE: 0.8602\n",
            "Epoch 15/25, RMSE: 0.8545\n",
            "Epoch 16/25, RMSE: 0.8489\n",
            "Epoch 17/25, RMSE: 0.8433\n",
            "Epoch 18/25, RMSE: 0.8377\n",
            "Epoch 19/25, RMSE: 0.8321\n",
            "Epoch 20/25, RMSE: 0.8267\n",
            "Epoch 21/25, RMSE: 0.8213\n",
            "Epoch 22/25, RMSE: 0.8159\n",
            "Epoch 23/25, RMSE: 0.8106\n",
            "Epoch 24/25, RMSE: 0.8055\n",
            "Epoch 25/25, RMSE: 0.8004\n"
          ]
        }
      ],
      "source": [
        "# MF + Bias + CF\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def MF_bias_ii(train_mat, num_factors=20, lr=0.005, reg=0.01, epochs=20, k=10, alpha=0.5, verbose=True):\n",
        "    num_users, num_items = train_mat.shape\n",
        "    # Initialize latent factors\n",
        "    User = np.random.normal(scale=0.1, size=(num_users, num_factors))\n",
        "    Item = np.random.normal(scale=0.1, size=(num_items, num_factors))\n",
        "    #Initialize bias\n",
        "    ub = np.zeros(num_users)\n",
        "    ib = np.zeros(num_items)\n",
        "    mu = np.mean(train_mat[train_mat != 0])\n",
        "    rows, cols = train_mat.nonzero()\n",
        "    ratings = train_mat[rows, cols]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(len(ratings)):\n",
        "            u = rows[i]\n",
        "            m = cols[i]\n",
        "            rating = ratings[i]\n",
        "            pred = mu + ub[u] + ib[m] + np.dot(User[u], Item[m])\n",
        "            error = rating - pred\n",
        "            # Update latent vectors using SGD\n",
        "            ub[u] += lr * (error - reg * ub[u])\n",
        "            ib[m] += lr * (error - reg * ib[m])\n",
        "            User[u] += lr * (error * Item[m] - reg * User[u])\n",
        "            Item[m] += lr * (error * User[u] - reg * Item[m])\n",
        "\n",
        "        if verbose:\n",
        "            MF_bias_pred = mu + ub[:, None] + ib[None,:] + User @ Item.T\n",
        "            rmse = np.sqrt(np.mean((train_mat[rows, cols] - MF_bias_pred[rows, cols])**2))\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, RMSE: {rmse:.4f}\")\n",
        "\n",
        "    MF_bias_pred = mu + ub[:, None] + ib[None,:] + User @ Item.T\n",
        "\n",
        "    item_sim = cosine_similarity(train_mat.T)\n",
        "    MF_bias_ii_pred = MF_bias_pred.copy()\n",
        "    for user in range(num_users):\n",
        "        rated_items = np.where(train_mat[u]>0)[0]\n",
        "        for movie in range(num_items):\n",
        "            sim_scores = item_sim[movie][rated_items]\n",
        "            ratings = train_mat[user][rated_items]\n",
        "\n",
        "            if len(sim_scores) == 0 or np.sum(np.abs(sim_scores)) == 0:\n",
        "                cf_adjust = 0\n",
        "            else:\n",
        "                top_k_idx = np.argsort(sim_scores)[-k:]\n",
        "                top_k_sims = sim_scores[top_k_idx]\n",
        "                top_k_ratings = ratings[top_k_idx]\n",
        "                cf_adjust = np.dot(top_k_sims, top_k_ratings) / np.sum(np.abs(top_k_sims) + 1e-8)\n",
        "\n",
        "            MF_bias_ii_pred[user, movie] = alpha * MF_bias_pred[user, movie] + (1 - alpha) * cf_adjust\n",
        "\n",
        "    return MF_bias_ii_pred\n",
        "\n",
        "# mf_bias_ii_pred = MF_bias_ii(train_mat, num_factors=20, k=10, alpha=0.7)\n",
        "mf_bias_ii_pred = MF_bias_ii(\n",
        "    train_mat,\n",
        "    num_factors=40,\n",
        "    lr=0.005,\n",
        "    reg=0.05,\n",
        "    epochs=25,\n",
        "    k=10,\n",
        "    alpha=0.9\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0T_i6-ZvTuT",
        "outputId": "2172f6a7-deac-4ae8-ff2a-17e220157562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE of the MF: 0.9029\n"
          ]
        }
      ],
      "source": [
        "sq_error = (mf_bias_ii_pred[test_mat.nonzero()] - test_mat[test_mat.nonzero()]) ** 2\n",
        "rmse = np.sqrt(np.mean(sq_error))\n",
        "print(f\"RMSE of the MF: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1wBZn_LnVwf"
      },
      "source": [
        "\n",
        "From the above results, we can observe that Matrix Factorization with bias achieved the lowest RMSE (0.8714), making it the most accurate model. Slightly behind it was Matrix Factorization with bias and Collaborative Filtering (0.9029), followed by standard Matrix Factorization (0.9103), which still performed well but lacked the additional bias terms. The baseline Collaborative Filtering approach with bias (0.9340) showed moderate success, while Item-Item KNN (0.9441) was less accurate, potentially due to sparsity or insufficient overlap in user ratings. Finally, the non-personalized method resulted in the highest RMSE (0.979318), reflecting its inability to tailor recommendations to individual user preferences.\n",
        "\n",
        "*   Non-personalzied recommendation algorithm -  0.979318\n",
        "*   Item-item collaboration with KNN - 0.9441\n",
        "*   Baseline model (CF with bias) - 0.9340\n",
        "*   Matrix Factorization - 0.9103\n",
        "*   Matrix Factorization with bias and Collaborative filtering - 0.9029\n",
        "*   Matrix Factorization with bias - 0.8714\n",
        "\n",
        "MF with biases gave the best result, perhaps because it effectively blends latent factor modeling with user and item bias corrections, capturing both deeper relationships and consistent rating tendencies. By contrast, adding CF signals could help but wasn’t fine-tuned enough, plain MF lacks bias terms, baseline CF with bias mainly adjusts global averages, item-item KNN struggles with sparse data, and the non-personalized approach ignores individual preferences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIVwyb7gnVwf"
      },
      "source": [
        "# Part 3. Dual Embedding Space Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHguEjRnAM8e",
        "outputId": "8960af47-bd6a-4758-ad8d-3e9acaa549c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.11/dist-packages (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.24.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0nbWetSxQGJ",
        "outputId": "66ad7ecc-cfe1-45c8-a70c-3d95e0057e44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.24.3)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, gensim\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 scipy-1.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "amGGbmKv2WeP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "filepath = \"/content/drive/MyDrive/ML_633/enron_814\"\n",
        "docs = []\n",
        "\n",
        "for filename in sorted(os.listdir(filepath)):\n",
        "    path = os.path.join(filepath, filename)\n",
        "    with open(path, 'r') as file:\n",
        "        raw_text = file.read().lower()\n",
        "        parts = raw_text.split()\n",
        "        doc_id_raw = parts[1]\n",
        "        doc_id = doc_id_raw.strip(\"<>\").split('.')\n",
        "        doc_id = f\"{doc_id[0]}.{doc_id[1]}\"\n",
        "        lines = raw_text.splitlines()\n",
        "        content_lines = []\n",
        "        found_start = False\n",
        "\n",
        "        for line in lines:\n",
        "            if 'jarnold.nsf' in line:\n",
        "                found_start = True\n",
        "                continue\n",
        "            if found_start:\n",
        "                content_lines.append(line.strip())\n",
        "        if not found_start:\n",
        "            content_lines = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "        content = ' '.join(content_lines).split()\n",
        "\n",
        "        docs.append({\n",
        "            'Document-ID': doc_id,\n",
        "            'content': content\n",
        "        })\n",
        "\n",
        "sentences = [doc['content'] for doc in docs]\n",
        "model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def get_average_embedding(words):\n",
        "    vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
        "\n",
        "doc_embeddings = np.array([get_average_embedding(doc['content']) for doc in docs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PDLFt61nVwf"
      },
      "source": [
        "Now show the results for the query: `buyer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG44ipjFnVwf",
        "outputId": "8103dc2f-203e-4459-98a5-3fedc3b65ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: buyer\n",
            "Rank\tScore\t\tDocumentID\t\tDocument\n",
            "1\t0.9869\t190536.1075857652345\tgot tix for tonight\n",
            "2\t0.9868\t2799094.1075857601603\ttickets requisitioned for england/germany. $1500!!!!!!\n",
            "3\t0.9867\t12216704.1075857601669\tplease use this vol curve for a dry run to figure out var for my book, ng price, and jim's book, storage, and communicate the results. thanks,john\n",
            "4\t0.9867\t23808285.1075857598702\ti'm here... \"cooper, sean\" <coopers@epenergy.com> on 08/29/2000 01:27:32 pm to: cc: subject: pab deleted my pab file, or for the non technical among you, my outlook personal address book was accidently deleted this week in an upgrade to windows 2000 nt. i have restored an old one, but it is several months, if not a whole year out of date. this is the first message to confirm the current address i have for you is still active. please reply confirming you recieved it. a second message will follow to try and replace some of the address's i know i have lost. thanks for your help sean. ****************************************************************** this email and any files transmitted with it from el paso energy corporation are confidential and intended solely for the use of the individual or entity to whom they are addressed. if you have received this email in error please notify the sender. ******************************************************************\n",
            "5\t0.9867\t26669022.1075857656732\tcute girlfriends.... i'm in\n"
          ]
        }
      ],
      "source": [
        "query = 'buyer'\n",
        "query_embedding = get_average_embedding([query])\n",
        "scores = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
        "top_5_idx = scores.argsort()[::-1][:5]\n",
        "\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(\"Rank\\tScore\\t\\tDocumentID\\t\\tDocument\")\n",
        "for rank, idx in enumerate(top_5_idx, start=1):\n",
        "    doc_id = docs[idx]['Document-ID']\n",
        "    text = ' '.join(docs[idx]['content'])\n",
        "    print(f\"{rank}\\t{scores[idx]:.4f}\\t{doc_id}\\t{text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQBT9DpTnVwg"
      },
      "source": [
        "Now show the results for the query: `margins`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d5Uzq7xnVwg",
        "outputId": "5d4f9bd7-10a2-4354-9144-6b553b7c7d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: margins\n",
            "Rank\tScore\t\tDocumentID\t\tDocument\n",
            "1\t0.9818\t25081271.1075857600086\thello, just checking to see if things are progressing as scheduled. thanks, john from: vladimir gorny 07/31/2000 06:35 pm to: john j lavorato/corp/enron@enron, jeffrey a shankman/hou/ect@ect, john arnold/hou/ect@ect, debbie r brackett/hou/ect@ect, frank hayden/corp/enron@enron, stephen stock/hou/ect@ect cc: subject: forward-forward vol implementation plan plan of action for implementation of the var methodology change related to forward-forward volatilities: 1. finalize the methodology proposed (research/market risk) - done 2. testing of the new methodology for the natural gas desk in excel (market risk) - done 3. get approval for the methodology change from rick buy (see draft of the memo attached) - john lavorato and john sherriff - by 8/7/00 - john lavorato, any comments on the memo? - would you like to run this by john sherriff or should i do it? 4. develop and implement the new methodology in a stage environment (research/it) - by 8/14/00 5. test the new methodology (market risk, traders) - by 8/27/00 6. migrate into production (research/it) - 8/28/00 please let me know if this is reasonable and meets everyone's expectations. vlady.\n",
            "2\t0.9818\t11256811.1075857601060\tmargaret: as you can imagine, most information and procedures on the floor are extremely confidential. we look at the gas market uniquely and using different tools than anybody else. it is one of our competitive advantages. i'm hesistant to approve the use of any documents for external purposes. if you provide more information about what you're trying to show, who the target audience is, and what format it will be presented, i may be able to help you. john margaret allen@enron 07/13/2000 11:09 am to: ann m schmidt/corp/enron@enron, john arnold/hou/ect@ect cc: subject: screen shots did you get this? my computer registered that it didn't go....delete if you already did. ---------------------- forwarded by margaret allen/corp/enron on 07/13/2000 11:03 am --------------------------- margaret allen 07/13/2000 10:45 am to: ann m schmidt/corp/enron@enron, john arnold/hou/ect@ect cc: subject: screen shots john, please look over this file and let me know if it is okay for us to use it as the screen shot that is on all the monitors in the commercial. please include ann on your response, as she will be passing it through legal since i'm out of the office. ann, once he gets this back to you, please run it by mark taylor. if he is okay with it, email the final version back to me, because the crew in toyko needs it asap. thanks! margaret ---------------------- forwarded by margaret allen/corp/enron on 07/13/2000 10:35 am --------------------------- from: kal shah @ ect 07/13/2000 09:17 am to: margaret allen/corp/enron@enron cc: subject: screen shots margaret -- it's critical that you get john arnold's permission before using the attached spreadsheet and graphs. they contain curves through july 12th. also, you may want to get legal permission from mark taylor. kal ---------------------- forwarded by kal shah/hou/ect on 07/13/2000 09:12 am --------------------------- heather alon 07/13/2000 09:08 am to: kal shah/hou/ect@ect cc: subject: screen shots hi kal, here are some screen shots, let me know if they will work for you. i think we may need to double check with john arnold- the trader- before using them for sure. but i was told they would be okay. heather\n",
            "3\t0.9818\t28703589.1075857655631\t---------------------- forwarded by john arnold/hou/ect on 02/26/2001 08:30= =20 pm --------------------------- from: mark frevert/enron@enronxgate on 02/23/2001 01:12 pm to: jeffery ader/hou/ect@ect, berney c aucoin/hou/ect@ect, edward d=20 baughman/hou/ect@ect, dana davis/enron@enronxgate, doug=20 gilbert-smith/corp/enron@enron, rogers herndon/hou/ect@ect, ben=20 jacoby/hou/ect@ect, ozzie pagan/hou/ect@ect, kevin m presto/hou/ect@ect,=20 fletcher j sturm/hou/ect@ect, bruce sukaly/corp/enron@enron, lloyd=20 will/hou/ect@ect, mark tawney/enron@enronxgate, george mcclellan/hou/ect@ec= t,=20 fred lagrasta/hou/ect@ect, john arnold/hou/ect@ect, scott neal/hou/ect@ect,= =20 hunter s shively/hou/ect@ect, phillip k allen/hou/ect@ect, thomas a=20 martin/hou/ect@ect cc: =20 subject: fw: \"chinese wall\" classroom training chinese wall training of one hour has been scheduled on the dates listed=20 below. the training is mandatory and allows ews to continue operating all= =20 its businesses including equity trading without violating the securities la= ws. please register for one of the four one-hour sessions listed below. each= =20 session is tailored to a particular commercial group, and it would be=20 preferable if you could attend the session for your group. (your particula= r=20 group is the one highlighted in bold on the list below.) =20 monday, march 5, 2001, 10:00 a.m. =01) resource group monday, march 5, 2001, 11:00 a.m. =01) origination/business development monday, march 5, 2001, 3:30 p.m. =01) financial trading group monday, march 5, 2001, 4:30 p.m. =01) heads of trading desks each of the above sessions will be held at the downtown hyatt regency hotel= =20 in sandalwood rooms a & b. alternatively, two make-up sessions are schedul= ed=20 for tuesday, march 13, 2001 at 3:30 p.m. and 4:30 p.m. location informatio= n=20 for the make-up sessions will be announced later. please confirm your attendance at one of these sessions with brenda whitehe= ad=20 by e-mailing her at brenda.whitehead@enron.com or calling her at extension= =20 3-5438. mark frevert and mark haedicke =20\n",
            "4\t0.9817\t15299653.1075857662533\t---------------------- forwarded by john arnold/hou/ect on 09/29/2000 03:31 pm --------------------------- from: beth miertschin 09/29/2000 02:31 pm to: jeffrey mcmahon/hou/ect@ect, john arnold/hou/ect@ect, brian steinbrueck/aa/corp/enron@enron, rick buy/hou/ect@ect, barry schnapper/corp/enron@enron, katie stowers/hou/ect@ect, nicole alvino/hou/ect@ect, russell t kelley/hou/ect@ect cc: sue ford/hou/ect@ect subject: vanderbilt presentation open presentation monday, october 2nd - 6:00 pm alumni hall, room 203 please meet in the lobby of the hotel at 5:15 pm or at the room by 5:30 pm so we can set up and finalize the game plan. after the presentation we are going to have a dinner for targeted candidates and also a reception for the people who came to the presentation. you will be informed about where you need to participate on monday; for now just keep the time open. hotel rooms: lowe's vanderbilt plaza hotel - 615-320-1700 nicole alvino - #6953024 brian steinbrueck - #7964648 katie stowers - #9216151 beth miertschin - #415172 rusty kelley - #414434 thank you for helping out! please let me know if you need anything else or have questions. beth miertschin\n",
            "5\t0.9817\t22568151.1075857597757\tjoe: i just wanted to run this past you... john ---------------------- forwarded by john arnold/hou/ect on 09/27/2000 12:05 pm --------------------------- to: john arnold/hou/ect@ect cc: joseph deffner/hou/ect@ect, tim despain/hou/ect@ect subject: re: john: i don't mind cleaning up their books at quarter end. however,at year end i will want to keep the debt off of my books. as we approach year-end this year could you please coordinate with joe deffner so that we take advantage of the margin lines we have available in order to minimize the debt on our books. thanks, ben john arnold 09/26/2000 07:12 pm to: ben f glisan/hou/ect@ect cc: subject: ben: jeff shankman gave me your name. i have assumed jeff's old responsibilities as head of the natural gas derivatives trading group. our broker, edf man, supplies us with $50 million of margin financing every night. they are trying to clean up their books for end of quarter and/or year on sep 30. they have asked if we can post the $50 million overnight on the 30th. we did this last year as well. please advise, john\n"
          ]
        }
      ],
      "source": [
        "query = 'margins'\n",
        "query_embedding = get_average_embedding([query])\n",
        "scores = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
        "top_5_idx = scores.argsort()[::-1][:5]\n",
        "\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(\"Rank\\tScore\\t\\tDocumentID\\t\\tDocument\")\n",
        "for rank, idx in enumerate(top_5_idx, start=1):\n",
        "    doc_id = docs[idx]['Document-ID']\n",
        "    text = ' '.join(docs[idx]['content'])\n",
        "    print(f\"{rank}\\t{scores[idx]:.4f}\\t{doc_id}\\t{text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrOfSjpgnVwg"
      },
      "source": [
        "Now show the results for the query: `winter`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QleWABZtnVwg",
        "outputId": "c5e5c484-f6fc-4e90-9f5f-1a602156561d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: winter\n",
            "Rank\tScore\t\tDocumentID\t\tDocument\n",
            "1\t0.9995\t7036870.1075857652237\twhat's your view of crude from here over next 1-4 weeks?\n",
            "2\t0.9994\t21805512.1075857596240\trajib: the following are my bids for the asian option: gq 1 : .41 gq 2 : .63 gq 3 : .57\n",
            "3\t0.9994\t33025919.1075857594206\tsaw a lot of the bulls sell summer against length in front to mitigate margins/absolute position limits/var. as these guys are taking off the front, they are also buying back summer. el paso large buyer of next winter today taking off spreads. certainly a reason why the spreads were so strong on the way up and such a piece now. really the only one left with any risk premium built in is h/j now. it was trading equivalent of 180 on access, down 40+ from this morning. certainly if we are entering a period of bearish to neutral trade, h/j will get whacked. certainly understand the arguments for h/j. if h settles $20, that spread is probably worth $10. h 20 call was trading for 55 on monday. today it was 10/17. the market's view of probability of h going crazy has certainly changed in past 48 hours and that has to be reflected in h/j. slafontaine@globalp.com on 12/13/2000 04:15:51 pm to: slafontaine@globalp.com cc: john.arnold@enron.com subject: re:spreads mkt getting a little more bearish the back of winter i think-if we get another cold blast jan/feb mite move out. with oil moving down and march closer flat px wide to jan im not so bearish these sprds now-less bullish march april as well.\n",
            "4\t0.9994\t13342827.1075857601087\tgood job....i've got 10% of my portfolio in cgp. keep up the good work. \"cooper, sean\" <coopers@epenergy.com> on 07/11/2000 08:03:41 pm to: cc: subject: el paso energy corporation reports record second quarter earnings per share http://biz.yahoo.com/prnews/000711/tx_el_paso_2.html tuesday july 11, 5:42 pm eastern time company press release source: el paso energy corporation el paso energy corporation reports record second quarter earnings per share houston, july 11 /prnewswire/ -- el paso energy corporation (nyse: epg <http://finance.yahoo.com/q?s=epg&d=t> - news </n/e/epg.html>) today announced second quarter 2000 adjusted diluted earnings per share of $0.69, an increase of 73 percent over second quarter 1999 adjusted diluted earnings per share of $0.40. the second quarter 2000 results exclude $0.13 per share of one-time merger-related items. diluted average common shares outstanding for the second quarter 2000 totaled 242 million. consolidated adjusted earnings before interest expense and income taxes (ebit) for the second quarter increased by 55 percent to $408 million, compared with $263 million in the year-ago period. ebit from the company's non-regulated businesses more than tripled in the quarter to $246 million, and represented 60 percent of consolidated ebit. ``outstanding growth in merchant energy and continued strong performance in our other non-regulated segments produced these record results,'' said william a. wise, president and chief executive officer of el paso energy corporation. ``reflecting our long-standing strategy of building a portfolio of flexible gas and power assets, merchant energy's earnings continued to accelerate in the second quarter.'' for the first six months of 2000, adjusted diluted earnings per share increased 84 percent to $1.40 per share, compared with $0.76 for the first six months of 1999. consolidated ebit for the six months, excluding non-recurring items, increased 55 percent to $798 million compared with $515 million in the year-ago period. second quarter business segment results the merchant energy segment reported record ebit of $152 million in the second quarter 2000, compared with $6 million in the same period last year and $50 million in the first quarter 2000. the physical and financial gas and power portfolio developed over the past several years is creating significant value in the current volatile energy environment. enhanced trading opportunities around our asset positions, continued strong wholesale customer business, and management fees from project electron (the company's off-balance sheet vehicle for power generation investments) all contributed to the record second-quarter performance. the production segment reported a 30-percent increase in second quarter ebit to $52 million compared with $40 million a year ago, reflecting higher realized gas and oil prices and lower operating costs following the reorganization of its business in 1999. weighted average realized prices for the quarter were $2.26 per million cubic feet (mmcf) of natural gas and $19.21 per barrel of oil, up 12 percent and 29 percent, respectively, from the year-ago levels. average natural gas production totaled 512 mmcf per day and oil production averaged 14,275 barrels per day. the field services segment reported second quarter ebit of $30 million, nearly double an adjusted $16 million in 1999. the increase was due primarily to higher realized gathering and processing margins, together with the acquisition of an interest in the indian basin processing plant in march 2000. second quarter gathering and treating volumes averaged 4.1 trillion btu per day (tbtu/d), while processing volumes averaged 1.1 tbtu/d. coming out of one of the warmest winters on record, the natural gas transmission segment reported second quarter ebit of $190 million compared with an adjusted $187 million a year ago, reflecting the realization of cost savings from the sonat merger. overall system throughput averaged 11.3 tbtu/d. during the quarter, southern natural gas received federal energy regulatory commission approval of its comprehensive rate case settlement filed in march. the international segment reported second quarter ebit of $12 million compared with $16 million in 1999. higher equity income from projects in brazil and argentina largely offset lower equity earnings from the company's investment in the philippines. telecom update ``we have made substantial progress in the development of our telecommunications business, el paso global networks,'' said william a. wise. ``reflecting our market-centric approach to developing new businesses, we have named greg g. jenkins, the current president of el paso merchant energy, to head our telecommunications business. our expertise in building businesses in rapidly commoditizing markets, as demonstrated by our merchant energy success, provides us with a key competitive entry point in the telecommunications marketplace.'' quarterly dividend the board of directors declared a quarterly dividend of $0.206 per share on the company's outstanding common stock. the dividend will be payable october 2, 2000 to shareholders of record as of the close of business on september 1, 2000. there were 237,786,853 outstanding shares of common stock entitled to receive dividends as of june 30, 2000. with over $19 billion in assets, el paso energy corporation provides comprehensive energy solutions through its strategic business units: tennessee gas pipeline company, el paso natural gas company, southern natural gas company, el paso merchant energy company, el paso energy international company, el paso field services company, and el paso production company. the company owns north america's largest natural gas pipeline system, both in terms of throughput and miles of pipeline, and has operations in natural gas transmission, merchant energy services, power generation, international project development, gas gathering and processing, and gas and oil production. on may 5, the stockholders of both el paso energy and the coastal corporation overwhelmingly voted in favor of merging the two organizations. the combined company will have assets of $35 billion and be one of the world's leading integrated energy companies. the merger is expected to close in the fourth quarter of this year, concurrent with the completion of regulatory reviews. visit el paso energy's web site at www.epenergy.com <http://www.epenergy.com>. cautionary statement regarding forward-looking statements this release includes forward-looking statements and projections, made in reliance on the safe harbor provisions of the private securities litigation reform act of 1995. the company has made every reasonable effort to ensure that the information and assumptions on which these statements and projections are based are current, reasonable, and complete. however, a variety of factors could cause actual results to differ materially from the projections, anticipated results or other expectations expressed in this release. while the company makes these statements and projections in good faith, neither the company nor its management can guarantee that the anticipated future results will be achieved. reference should be made to the company's (and its affiliates') securities and exchange commission filings for additional important factors that may affect actual results. el paso energy corporation consolidated statement of income (in millions, except per share amounts) (unaudited) second quarter ended six months ended june 30, june 30, 2000 1999 2000 1999 operating revenues $4,227 $2,597 $7,333 $4,875 operating expenses cost of gas and other products 3,451 1,949 5,829 3,590 operation and maintenance 226 223 436 469 merger related costs and asset impairment charges 46 131 46 135 ceiling test charges --- --- --- 352 depreciation, depletion, and amortization 148 141 293 289 taxes, other than income taxes 36 36 77 76 3,907 2,480 6,681 4,911 operating income (loss) 320 117 652 (36) equity earnings and other income 42 64 100 113 earnings before interest expense, income taxes, and other charges 362 181 752 77 interest and debt expense 127 110 250 212 minority interest 27 4 49 8 income (loss) before income taxes and other charges 208 67 453 (143) income tax expense (benefit) 68 23 142 (52) preferred stock dividends of subsidiary 6 6 12 12 income (loss) before extraordinary items and cumulative effect of accounting change 134 38 299 (103) extraordinary items, net of income taxes --- --- 89 --- cumulative effect of accounting change, net of income taxes --- --- --- (13) net income (loss) $134 $38 $388 $(116) diluted earnings (loss) per common share: adjusted diluted earnings per common share (a) $0.69 $0.40 $1.40 $0.76 extraordinary items --- --- 0.37 --- cumulative effect of accounting change --- --- --- (0.06) merger related costs, asset impairment, and other non-recurring charges (0.13) (0.36) (0.13) (0.36) ceiling test charges --- --- --- (0.94) gain on sale of assets --- 0.05 --- 0.05 resolution of regulatory issues --- 0.08 --- 0.08 proforma diluted earnings (loss) per common share $0.56 $0.17 $1.64 $(0.47)(b) reported diluted earnings (loss) per common share $0.56 $0.17 $1.64 $(0.51)(b) basic average common shares outstanding (000's) 229,539 226,877 229,064 226,471 diluted average common shares outstanding (000's) 241,710 237,955 240,117 237,161 (a) adjusted diluted earnings per common share represents diluted earnings per share before the impact of certain non-recurring charges. second quarter 2000 results exclude merger related charges of $(46) million pretax, or $(31) million aftertax. second quarter 1999 results exclude merger related charges of $(131) million pretax, or $(86) million aftertax, a gain on sale of assets of $19 million pretax, or $12 million aftertax, and the resolution of regulatory issues of $30 million pretax, or $20 million aftertax. year-to-date 2000 results exclude the extraordinary gain on the sale of the east tennessee and sea robin systems of $89 million aftertax and merger related charges of $(46) million pretax, or $(31) million aftertax. year-to-date 1999 results exclude the cumulative effect of an accounting change of $(13) million aftertax, merger related charges of $(135) million pretax, or $(86) million aftertax, ceiling test charges of $(352) million pretax, or $(222) million aftertax, a gain on sale of assets of $19 million pretax, or $12 million aftertax, and the resolution of regulatory issues of $30 million pretax, or $19 million aftertax. (b) proforma diluted loss per common share reflects reported diluted earnings per share but assumes dilution. reported diluted loss per common share does not assume dilution because dilution would reduce the amount of loss per share. source: el paso energy corporation ****************************************************************** this email and any files transmitted with it from el paso energy corporation are confidential and intended solely for the use of the individual or entity to whom they are addressed. if you have received this email in error please notify the sender. ******************************************************************\n",
            "5\t0.9994\t24506788.1075857656344\tyesterday, aquilla sold march at 5.77 and 5.76 for hehub. please change it to nymex\n"
          ]
        }
      ],
      "source": [
        "query = 'winter'\n",
        "query_embedding = get_average_embedding([query])\n",
        "scores = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
        "top_5_idx = scores.argsort()[::-1][:5]\n",
        "\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(\"Rank\\tScore\\t\\tDocumentID\\t\\tDocument\")\n",
        "for rank, idx in enumerate(top_5_idx, start=1):\n",
        "    doc_id = docs[idx]['Document-ID']\n",
        "    text = ' '.join(docs[idx]['content'])\n",
        "    print(f\"{rank}\\t{scores[idx]:.4f}\\t{doc_id}\\t{text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFGe4T8vnVwg"
      },
      "source": [
        "Now show the results for the query: `risk`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_uocGybnVwg",
        "outputId": "1cd4cad6-a4b0-4b52-afb8-a6b6ba6d1122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: risk\n",
            "Rank\tScore\t\tDocumentID\t\tDocument\n",
            "1\t0.9996\t32572339.1075857602061\t---------------------- forwarded by john arnold/hou/ect on 04/11/2000 04:57 pm --------------------------- from: rudi zipter 04/08/2000 09:03 am to: john arnold/hou/ect@ect cc: vladimir gorny/hou/ect@ect, minal dalia/hou/ect@ect, sunil dalal/corp/enron@enron subject: option analysis on ng price book john, several months ago we talked about the development of an option analysis tool that could be used to stress test positions under various scenarios as a supplement to our v@r analysis. we have recently completed the project and would like to solicit your feedback on the report results. we have selected your ng price position for april 4, 2000 (post-id 753650) for the initial analysis. attached in the excel file below you will find: analysis across the various forward months in your position underlying vs. greeks, theoretical p&l volatility vs. greeks, theoretical p&l time change vs. greeks, theoretical p&l summary of your overall position analysis underlying vs. greeks, theoretical p&l volatility vs. greeks, theoretical p&l time change vs. greeks, theoretical p&l multiple stress analysis the attached word document demonstrates the multiple stress choices. i have included a tab in the excel file that demonstrates the theoretical p/l resulting from shifts in both volatility and underlying price. please note that the percentage changes across the column headers are not in absolute terms (for example, if the atm volatility in a given month is 40% and the stress is -10% then the analysis is performed under a volatility scenario of 36%) thanks, rudi\n",
            "2\t0.9996\t25585212.1075857600864\tandy: is the new version of eol coming out soon. i'm waiting to put new products on until it comes... john\n",
            "3\t0.9996\t17350292.1075857654244\t71 george.ellis@americas.bnpparibas.com on 03/14/2001 08:18:25 am to: george.ellis@americas.bnpparibas.com cc: subject: bnp paribas commodity futures weekly aga survey good morning, just a reminder to get your aga estimates in by noon est (11:00 cst) today. last year -31 last week -73 thank you, george ellis bnp paribas commodity futures, inc. ______________________________________________________________________________ _______________________________________________________ ce message et toutes les pieces jointes (ci-apres le \"message\") sont etablis a l'intention exclusive de ses destinataires et sont confidentiels. si vous recevez ce message par erreur, merci de le detruire et d'en avertir immediatement l'expediteur. toute utilisation de ce message non conforme a sa destination, toute diffusion ou toute publication, totale ou partielle, est interdite, sauf autorisation expresse. l'internet ne permettant pas d'assurer l'integrite de ce message, bnp paribas (et ses filiales) decline(nt) toute responsabilite au titre de ce message, dans l'hypothese ou il aurait ete modifie. ------------------------------------------------------------------------------ ---- this message and any attachments (the \"message\") are intended solely for the addressees and are confidential. if you receive this message in error, please delete it and immediately notify the sender. any use not in accord with its purpose, any dissemination or disclosure, either whole or partial, is prohibited except formal approval. the internet can not guarantee the integrity of this message. bnp paribas (and its subsidiaries) shall (will) not therefore be liable for the message if modified. ______________________________________________________________________________ _______________________________________________________\n",
            "4\t0.9996\t19011432.1075857654758\t67 george.ellis@americas.bnpparibas.com on 03/07/2001 09:39:04 am to: george.ellis@americas.bnpparibas.com cc: subject: bnp paribas commodity futures weekly aga survey good morning, just a reminder to get your aga estimates in by noon est (11:00 cst) today. last year -37 last week -101 thanks, george ellis bnp paribas commodity futures, inc. ______________________________________________________________________________ _______________________________________________________ ce message et toutes les pieces jointes (ci-apres le \"message\") sont etablis a l'intention exclusive de ses destinataires et sont confidentiels. si vous recevez ce message par erreur, merci de le detruire et d'en avertir immediatement l'expediteur. toute utilisation de ce message non conforme a sa destination, toute diffusion ou toute publication, totale ou partielle, est interdite, sauf autorisation expresse. l'internet ne permettant pas d'assurer l'integrite de ce message, bnp paribas (et ses filiales) decline(nt) toute responsabilite au titre de ce message, dans l'hypothese ou il aurait ete modifie. ------------------------------------------------------------------------------ ---- this message and any attachments (the \"message\") are intended solely for the addressees and are confidential. if you receive this message in error, please delete it and immediately notify the sender. any use not in accord with its purpose, any dissemination or disclosure, either whole or partial, is prohibited except formal approval. the internet can not guarantee the integrity of this message. bnp paribas (and its subsidiaries) shall (will) not therefore be liable for the message if modified. ______________________________________________________________________________ _______________________________________________________\n",
            "5\t0.9996\t29486194.1075857596823\t82 michael.byrne@americas.bnpparibas.com on 10/11/2000 08:17:21 am to: michael.byrne@americas.bnpparibas.com cc: subject: bnp paribas commodity futures weekly aga survey good morning, just a reminder to get your aga estimates in by noon est (11:00 cst) today. last year +49 last week +78 thanks, michael byrne bnp paribas commodity futures ______________________________________________________________________________ _______________________________________________________ ce message et toutes les pieces jointes (ci-apres le \"message\") sont etablis a l'intention exclusive de ses destinataires et sont confidentiels. si vous recevez ce message par erreur, merci de le detruire et d'en avertir immediatement l'expediteur. toute utilisation de ce message non conforme a sa destination, toute diffusion ou toute publication, totale ou partielle, est interdite, sauf autorisation expresse. l'internet ne permettant pas d'assurer l'integrite de ce message, bnp paribas (et ses filiales) decline(nt) toute responsabilite au titre de ce message, dans l'hypothese ou il aurait ete modifie. ------------------------------------------------------------------------------ ---- this message and any attachments (the \"message\") are intended solely for the addressees and are confidential. if you receive this message in error, please delete it and immediately notify the sender. any use not in accord with its purpose, any dissemination or disclosure, either whole or partial, is prohibited except formal approval. the internet can not guarantee the integrity of this message. bnp paribas (and its subsidiaries) shall (will) not therefore be liable for the message if modified. ______________________________________________________________________________ _______________________________________________________\n"
          ]
        }
      ],
      "source": [
        "query = 'risk'\n",
        "query_embedding = get_average_embedding([query])\n",
        "scores = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
        "top_5_idx = scores.argsort()[::-1][:5]\n",
        "\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(\"Rank\\tScore\\t\\tDocumentID\\t\\tDocument\")\n",
        "for rank, idx in enumerate(top_5_idx, start=1):\n",
        "    doc_id = docs[idx]['Document-ID']\n",
        "    text = ' '.join(docs[idx]['content'])\n",
        "    print(f\"{rank}\\t{scores[idx]:.4f}\\t{doc_id}\\t{text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZKuVIWbnVwg"
      },
      "source": [
        "Now show the results for the query: `never`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XpZbxIXnVwg",
        "outputId": "0964d0f3-bd4b-47a2-b674-bc42c7effa84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: never\n",
            "Rank\tScore\t\tDocumentID\t\tDocument\n",
            "1\t0.9994\t3233370.1075857658589\tthat info is correct. from: edie leschber 12/29/2000 12:30 pm to: john arnold/hou/ect@ect cc: subject: gas team - reorg john, my name is edie leschber and i will be your business analysis and reporting contact effective immediately. i am currently in the process of verifying team members under your section of the gas team. attached is a file with the current list. please confirm that your list is complete and/or send me changes to it at your earliest convenience. new cost centers have been set up due to the reorganization and we would like to begin using these as soon as possible. i look forward to meeting you and working with you very soon. thank you for your assistance. edie leschber x30669\n",
            "2\t0.9994\t18917089.1075857653085\tas long as i own enron stock, the desks are my colleagues. feel free to share the info with hunter and chris. clayton vernon @ enron 03/26/2001 03:45 pm to: john arnold/hou/ect@ect cc: subject: ? about turf john- my name is clayton vernon, and i am the manager for models and forecasts for east power trading, reporting to lloyd will. i'm helping research (and toim barkley) with a data visualization tool for eol trades, and i wanted for you to know i'd like to make this product helpful for you. in doing so, i'd like to also let my colleagues chris gaskill/hunter shively avail themsevles of this tool. but, i need to find out if the desks are your colleagues or, as things shape up, your competitors, since some trades are between enron desks. can chris/hunter see synopses of all of our eol gas trades, after the fact? clayton vernon\n",
            "3\t0.9994\t25032864.1075857658040\ti said i'm a vp and run natural gas derivatives trading and she asked if she could eliminate derivatives because then she'd have to write two more sentences about what that was. \"jennifer white\" <jenwhite7@zdnetonebox.com> on 01/11/2001 01:13:11 pm to: john.arnold@enron.com cc: subject: there is no john taylor at ocean, and i don't remember the other name you mentioned on the phone. whoever you were talking to last night was confused. i found the article in the baltimore sun. since when are you vp of natural gas trading? i'm meeting the girls at saba tonight in the event you don't have plans and want to join us. jen ___________________________________________________________________ to get your own free zdnet onebox - free voicemail, email, and fax, all in one place - sign up today at http://www.zdnetonebox.com\n",
            "4\t0.9994\t30031046.1075857598528\tcome talk to me sometime after 4:00 from: jennifer fraser 09/11/2000 07:15 pm to: john arnold/hou/ect@ect cc: subject: tony harris ja; six months ago we spoke about tony moving to a commercial role. he followed your advice and has gained some experience in structuring. he approached me about joining the middle marketing group. i think he would an asset. i think we would bring him as an associate with the hope that he would move up to manager in 6 months. please let me know your thoughts. i would like to use you as a reference with fred and craig. thanks jf\n",
            "5\t0.9994\t32195351.1075857599027\tglad to see you're having so much fun with this. i've been here 5.5 years with nothing and then in one week i'm in fortune and time. pretty funny. things are going well here . the big e just chugging along, bringing the stock price with it. wish i could tell you everything new in my life, but i think i just did. your long lost buddy, john heather robertson <hrobertson@hbk.com> on 08/31/2000 10:10:09 am to: john.arnold@enron.com cc: subject: young john? is this \"the 26-year-old chief of natural-gas derivatives\"? if so, you should write me back after you complete your \"nine hours and $1 billion in trades\" today!! does this mean i have to start calling you \"mr. john\"?! it was great to see you're doing so well.... your long lost buddy, heather heather lockhart robertson hbk investments lp personal: 214.758.6161 phone 214.758.1261 fax investor relations: 214.758.6108 phone 214.758.1208 fax\n"
          ]
        }
      ],
      "source": [
        "query = 'never'\n",
        "query_embedding = get_average_embedding([query])\n",
        "scores = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
        "top_5_idx = scores.argsort()[::-1][:5]\n",
        "\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(\"Rank\\tScore\\t\\tDocumentID\\t\\tDocument\")\n",
        "for rank, idx in enumerate(top_5_idx, start=1):\n",
        "    doc_id = docs[idx]['Document-ID']\n",
        "    text = ' '.join(docs[idx]['content'])\n",
        "    print(f\"{rank}\\t{scores[idx]:.4f}\\t{doc_id}\\t{text}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
